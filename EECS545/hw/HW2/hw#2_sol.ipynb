{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hw #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) <br>\n",
    "f '(x) = $e^x$ which is increasing in x $\\in$ and R is a convex set. <br>\n",
    "Therefore, f(x) = $e^x - 1$ is strictly convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) <br>\n",
    "$H = \\begin{pmatrix} \\mathbf{0} & \\mathbf{1} \\\\ \\mathbf{1} & \\mathbf{0} \\end{pmatrix} $ <br>\n",
    "Let $x = (x_1, x_2)^T$ <br>\n",
    "Then $x^THx = (x_1, x_2) \\begin{pmatrix} \\mathbf{0}&\\mathbf{1}\\\\ \\mathbf{1}&\\mathbf{0}\\end{pmatrix} \\begin{pmatrix}\\mathbf{x_1}\\\\ \\mathbf{x_2}\\end{pmatrix} = 2x_1x_2 > 0$ since $x_1$, $x_2 \\in R_{++}^2$ <br> Also, $R_{++}^2$ is a convex set. <br>\n",
    "Therefore,$f(x_1, x_2) = x_1x_2$ is strictly convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) <br>\n",
    "$H = \\alpha(1-\\alpha)\\begin{pmatrix}\\mathbf{-x_1^{\\alpha-2}x_2^{1-\\alpha}}&\\mathbf{x_1^{\\alpha-1}x_2^{-\\alpha}} \\\\ \\mathbf{x_1^{\\alpha-1}x_2^{-\\alpha}}&\\mathbf{-x_1^{\\alpha}x_2^{-\\alpha-1}} \\end{pmatrix}$ <br>\n",
    "Let $y = (y_1, y_2)^T$<br>\n",
    "Then $y^THy = (y_1, y_2)*\\alpha(1-\\alpha)\\begin{pmatrix}\\mathbf{-x_1^{\\alpha-2}x_2^{1-\\alpha}}&\\mathbf{x_1^{\\alpha-1}x_2^{-\\alpha}} \\\\ \\mathbf{x_1^{\\alpha-1}x_2^{-\\alpha}}&\\mathbf{-x_1^{\\alpha}x_2^{-\\alpha-1}} \\end{pmatrix} \\begin{pmatrix}\\mathbf{y_1} \\\\ \\mathbf{y_2}\\end{pmatrix}$<br>\n",
    "$= \\alpha(1-\\alpha)x_1^{\\alpha-1}x_2^{-\\alpha}[-y_1^2 \\frac{x_2}{x_1}-y_2^2 \\frac{x_1}{x_2}+2y_1y_2] <= \\alpha(1-\\alpha)x_1^{\\alpha-1}x_2^{-\\alpha}[-2\\sqrt{y_1^2y_2^2}+2y_1y_2] <= 0$<br> Since $x_1, x_2 \\in R_{++}^2$ and $0\\leq \\alpha \\leq 1$ and equality holds iff $x_1 = x_2$ <br> Therefore, $f(x_1, x_2) = x_1^{\\alpha}x_2^{1-\\alpha}$ is positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) <br>\n",
    "#### (i) <br>\n",
    "$J(\\theta) = \\frac{1}{2}||A^T\\theta - y||^2 = \\frac{1}{2}(A^T\\theta - y)^T(A^T\\theta - y) = \\frac{1}{2}\\theta^TAA^T\\theta - 2y^TA^T\\theta + y^Ty$\n",
    "\n",
    "$\\nabla_{\\theta} J(\\theta) = AA^T\\theta - Ay$<br>\n",
    "The update rule: <br>\n",
    "$\\theta^{t+1} = \\theta^{t} - \\alpha (AA^T\\theta - Ay)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) <br>\n",
    "According to HW#1 Problem(4), $J(\\theta)$ is convex. <br>\n",
    "Also, according to property 6: J is convex, continuous differetiable, then $x*$ is a global minimizer iff $\\nabla_{\\theta}J(\\theta) = 0$ <br>\n",
    "Thus, we have $\\nabla_{\\theta} J(\\theta) = AA^T\\theta - 2Ay = 0$ <br>\n",
    "$\\theta = (AA^T)^{-1}Ay$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myNaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for spam classification\n",
    "    Using the multinomial event model and Laplace smoothing\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.words_with_labels = []\n",
    "        self.prior = []\n",
    "        \n",
    "    def get_bag_of_words_model(self):\n",
    "        return self.words_with_labels\n",
    "    \n",
    "    def calc_log_prior(self, y_train):\n",
    "        \"\"\"\n",
    "        calculate the prior probability of label y\n",
    "        \"\"\"\n",
    "        numSpam = sum([ 1 for i in y_train if i== 1])\n",
    "        PrSpam = float(numSpam) / len(y_train)\n",
    "        PrNonSpam = 1 - PrSpam\n",
    "        return [math.log(PrSpam), math.log(PrNonSpam)]\n",
    "    \n",
    "    def calc_conditional_prob_of_words(self, X_train,y_train):\n",
    "        \"\"\"\n",
    "        calculate the conditional probability of word wj given label y\n",
    "        \"\"\"\n",
    "        result = [[0]*len(X_train[0]), [0]*len(X_train[0])]\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            for j in range(len(X_train[0])):\n",
    "                if(y_train[i] == 1): # spam\n",
    "                    result[0][j] += float(X_train[i][j])\n",
    "                else: # non-spam\n",
    "                    result[1][j] += float(X_train[i][j])\n",
    "                    \n",
    "        for j in range(len(X_train[0])):\n",
    "            result[0][j] = (1.0 + result[0][j]) / (sum(np.add(result[0],1.0)))#+len(X_train[0]))\n",
    "            result[1][j] = (1.0 + result[1][j]) / (sum(np.add(result[1],1.0)))#+len(X_train[0]))\n",
    "        return result\n",
    "    \n",
    "    def calc_log_posterior(self, X_test):\n",
    "        \"\"\"\n",
    "        calculate log of Pr(di|yi)\n",
    "        \"\"\"\n",
    "        res = np.zeros((X_test.shape[0],2))\n",
    "\n",
    "        for i in range(X_test.shape[0]):\n",
    "            for j in range(len(X_test[0])):\n",
    "                # probability the email is spam\n",
    "                res[i][0] += X_test[i][j] * math.log(self.words_with_labels[0][j])\n",
    "                # probability the email is non-spam\n",
    "                res[i][1] += X_test[i][j] * math.log(self.words_with_labels[1][j])\n",
    "        return res\n",
    "                   \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        train the bag of words model\n",
    "        \"\"\"\n",
    "        self.prior = self.calc_log_prior(y_train)\n",
    "        self.words_with_labels = self.calc_conditional_prob_of_words(X_train, y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        predict whether given emails are spam or not\n",
    "        \"\"\"\n",
    "        pred = [0] * X_test.shape[0]\n",
    "        tmp = self.calc_log_posterior(X_test)\n",
    "        \n",
    "        for i in range(X_test.shape[0]):\n",
    "            tmp[i] += tmp[i] + self.prior\n",
    "            pred[i] = 1 if tmp[i][0] > tmp[i][1] else -1\n",
    "        return pred\n",
    "    \n",
    "    def error_rate(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        calculate the error rate\n",
    "        error_rate = # of wrongly classified documents in SPARSE.TEST / # of document in SPARSE.TEST * 100%\n",
    "        \"\"\"\n",
    "        \n",
    "        return float(sum([1 for i in range(len(y_pred))if y_pred[i] != y_true[i]])) / len(y_true) * 100\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate on the test set is 1.625%\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "train = sio.loadmat('./spam_classification_matlab/train.mat')\n",
    "test = sio.loadmat('./spam_classification_matlab/test.mat')\n",
    "\n",
    "X_train = train['DENSE_TRAIN']\n",
    "y_train = train['DENSE_Y_TRAIN']\n",
    "\n",
    "X_test = test['DENSE_TEST_X']\n",
    "y_true = test['DENSE_TEST_Y']\n",
    "\n",
    "\n",
    "NB = myNaiveBayesClassifier()\n",
    "\n",
    "NB.fit(X_train, y_train)\n",
    "\n",
    "pred = NB.predict(X_test)\n",
    "err = NB.error_rate(pred, y_true)\n",
    "print(\"The error rate on the test set is \" + str(err) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five tokens that most indicative of the SPAM class are \n",
      "websit\n",
      "valet\n",
      "unsubscrib\n",
      "spam\n",
      "httpaddr\n"
     ]
    }
   ],
   "source": [
    "import queue as q\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./spam_classification_matlab/TOKENS_LIST\",sep='\\s+', names=['index','value'])\n",
    "\n",
    "dic = df['value']\n",
    "# print(dic[:][2])\n",
    "\n",
    "bag_of_words = NB.get_bag_of_words_model()\n",
    "\n",
    "pq = q.PriorityQueue()\n",
    "\n",
    "for i in range(len(bag_of_words[0])):\n",
    "    tmp = (bag_of_words[0][i] / bag_of_words[1][i],i)\n",
    "    pq.put(tmp)\n",
    "    if pq.qsize() > 5:\n",
    "        pq.get()\n",
    "\n",
    "print(\"The five tokens that most indicative of the SPAM class are \")\n",
    "lst = [-1] * 5\n",
    "for i in range(pq.qsize()):\n",
    "    lst[i] = pq.get()[1]\n",
    "    print(dic[lst[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import queue as q # priority queue\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_data = sio.loadmat('mnist_data.mat')\n",
    "\n",
    "# training data\n",
    "train = mnist_data['train']\n",
    "X_train = train[0:,1:]\n",
    "y_train = train[0:,0]\n",
    "\n",
    "# testing data\n",
    "test = mnist_data['test']\n",
    "X_test = test[0:,1:]\n",
    "y_test = test[0:,0]\n",
    "print(\"successfully loading the training and testing data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myKNNClassifier:\n",
    "    def __init__(self, n_neighbors):\n",
    "        \"\"\"\n",
    "        define number of classes\n",
    "        \"\"\"\n",
    "        self.k_neighbors = n_neighbors\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        The training phase of the algorithm consists only\n",
    "        of storing the feature vectors and class labels \n",
    "        of the training samples\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, X_test, lxnorm):\n",
    "        \"\"\"\n",
    "        calculate the K nearest points based on different norm \n",
    "        e.g. l1-norm, l2-norm\n",
    "    \n",
    "        Input: 100 test images\n",
    "        Output: the corresponding predicated label for each test image \n",
    "        \n",
    "        Complexity: O( n*log(k) ) where n is the number of training\n",
    "        examples and k is the number of nearest neighbours, multiplied\n",
    "        by the number testing images, here p = 100.\n",
    "        \n",
    "        Using priority queue implementation\n",
    "        \"\"\"\n",
    "        \n",
    "        res = []\n",
    "        # print(res)\n",
    "        \n",
    "        for i in range(X_test.shape[0]):\n",
    "            ### find the k nearest points for each testing image ###\n",
    "            norm = np.linalg.norm(X_test[i]-self.X_train,lxnorm,axis=1)\n",
    "            mypq = q.PriorityQueue()\n",
    "            for index, val in enumerate(norm):\n",
    "                lst = [-val, index]\n",
    "                mypq.put(lst)\n",
    "                if(mypq.qsize() > self.k_neighbors):\n",
    "                    mypq.get()\n",
    "            tmp = []\n",
    "            for i in range(mypq.qsize()):\n",
    "                item = mypq.get()\n",
    "                tmp.append(self.y_train[item[1]])\n",
    "                \n",
    "            res.append(stats.mode(tmp)[0][0])\n",
    "            # print(res[i])\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def accuracy_score(self, y_pred, y_test):\n",
    "        \"\"\"\n",
    "        return the classification accuracy\n",
    "        accuracy = # correctly classified test images /  # total test images\n",
    "        \"\"\"\n",
    "        return float(sum([1 for i in range(len(y_pred))if y_pred[i] == y_test[i]])) / len(y_test) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8376 6884 1414 5668 7397 4901  640 4649 2186 4491]\n"
     ]
    }
   ],
   "source": [
    "# random sample 100 test images\n",
    "sample_index = np.random.choice(test.shape[0],100)\n",
    "X_test_sample = X_test[sample_index]\n",
    "y_test_sample = y_test[sample_index]\n",
    "print(sample_index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When k = 1 the accuracy score is 96.0%\n",
      "When k = 5 the accuracy score is 97.0%\n",
      "When k = 9 the accuracy score is 94.0%\n",
      "When k = 13 the accuracy score is 95.0%\n"
     ]
    }
   ],
   "source": [
    "K = [1,5,9,13]\n",
    "for k in K:\n",
    "    # find the accuracy score for each k and find the best k value\n",
    "    KNN = myKNNClassifier(k)\n",
    "    KNN.fit(X_train, y_train)\n",
    "    y_pred = KNN.predict(X_test_sample,2)# l2-norm\n",
    "\n",
    "    acc = KNN.accuracy_score(y_pred, y_test_sample)\n",
    "    print(\"When k = \" + str(k) + \" \" + \"the accuracy score is \" + str(acc) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results showed that when k = 5 had the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When k = 1 the accuracy score is 95.0%\n",
      "When k = 5 the accuracy score is 95.0%\n",
      "When k = 9 the accuracy score is 92.0%\n",
      "When k = 13 the accuracy score is 94.0%\n"
     ]
    }
   ],
   "source": [
    "K = [1,5,9,13]\n",
    "for k in K:\n",
    "    # find the accuracy score for each k and find the best k value\n",
    "    KNN = myKNNClassifier(k)\n",
    "    KNN.fit(X_train, y_train)\n",
    "    y_pred = KNN.predict(X_test_sample,1) # using the l1-norm\n",
    "\n",
    "    acc = KNN.accuracy_score(y_pred, y_test_sample)\n",
    "    print(\"When k = \" + str(k) + \" \" + \"the accuracy score is \" + str(acc) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results showed that when k = 1 or 5 had the best performance. I will choose the $l_2$ norm distance metric for MNIST since the accuracy score for $l_2$ norm is higher."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
